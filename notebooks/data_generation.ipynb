{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data \n",
    "This notebook goes over sampling methodology for our [recent work.](https://arxiv.org/abs/2407.05664), which mainly focusing on approximating composition chain of Sobolev functions.\n",
    "To sample synthetic data, we used Mat√©rn kernels whose smoothness and regularity properties closely aligns well with properties of Sobolev Space. Please check Corollary A.6 from [Tuo & Wu (2015)](https://arxiv.org/pdf/1508.07155)\n",
    "\n",
    "The eq. for Matern kernel follows:\n",
    "$$K(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\left(\n",
    "         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\n",
    "         \\right)^\\nu K_\\nu\\left(\n",
    "         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\right)$$\n",
    "\n",
    "As we mentioned in our paper on Appendix, Mat√©rn kernel is considered as a generalization of RBF kernel. With the differentiability parameter $\\nu$, it interpolates from 0 time differentiable kernel (Laplace Kernel) to infinitely differentiable or smooth kernel (Gaussian Kernel or RBF Kernel). The differentiability parameter $\\nu$ is simply the number of differentiability of kernel + 0.5; that is, the Mat√©rn kernel would be 2.3 times differentiable if $\\nu=2.8$.\n",
    "\n",
    "Notice that Mat√©rn kernel enables us to find even non-integer times differentiable. Thanks to the modified Bessel function :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages \n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import os \n",
    "\n",
    "from tqdm import tqdm\n",
    "from MaternKernel import matern_Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation `MaternKernel.py` is based on Mat√©rn kernel implementation from `sklearn`, an open-source machine learning library. You may use `sklearn` instead of our code or even `torch`. Both libraries provide Mat√©rn kernel implementation along with sampling components. We made that file just for brevity. Email me if you need either `sklearn` or `torch` version of this process.\n",
    "\n",
    "However, we do not recommend using `gpytorch`, another open source library focusing on running Gaussian Process in PyTorch environment. I cannot doubt `gpytorch` is a great library, but it does not provide various range of diffrentiability: only 0, 1, 2. Remind that our work need to test differentiablility from 0.5 to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "The following is the general procedure to sample data from Mat√©rn kernel: \n",
    "1. Sample standard multivariate normal distribution $X \\in \\mathbb{R}^{N \\times d_{in}}$\n",
    "2. From $X$, compute the $N \\times N$ kernel $K$ with given $\\nu$\n",
    "3. Sample $Y \\in\\mathbb{R}^{N \\times d_{out}}$ with columns sampled from the Gaussian $\\mathcal{N}(0,K)$.\n",
    " \n",
    "where $N$ is the number of dataset; $d_{in}$ and $d_{in}$ are dimension of $X$ and $Y$, resp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:06:21 AM:INFO:Sampling Matern Kernel with nu: 15\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 10484.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 15), (1000, 20))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize variables and sample GP X\n",
    "N = 1000\n",
    "d_in = 15\n",
    "d_out = 20\n",
    "nu = 15\n",
    "\n",
    "X = np.random.randn(N, d_in) #1.\n",
    "Y = matern_Kernel(X, nu = nu).sample(d_out) #2. & 3.\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Now all you have to do is to save `X` and `Y`. We saved each in pickle format with `N` and `nu`\n",
    "\n",
    "But wait. Didn't we just said our paper focus on the composition of Sobolev functions? \n",
    "Yes, this is just a way to sample data from a **single** kernel. \n",
    "\n",
    "Now, we will find composition! Our new procedure for the composition of two Mat√©rn kernel($K_g$, $K_h$) follows: \n",
    "\n",
    "1. Sample the training dataset $X \\in \\mathbb{R}^{N \\times d_{in}}$   \n",
    "2. From X, compute the $N \\times N$ kernel $K_g$ with given $\\nu_g$\n",
    "3. From $K_g$, sample $Z \\in\\mathbb{R}^{N \\times d_{mid}}$ with columns sampled from the Gaussian $\\mathcal{N}(0,K_g)$.\n",
    "4. From $Z$, compute $K_g$ with given $\\nu_h$\n",
    "5. From $K_h$, sample the test dataset $Y\\in\\mathbb{R}^{N \\times d_{out}}$ with columns sampled from the Gaussian $\\mathcal{N}(0,K_h)$.\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "Why only two kernels tho? It's because:\n",
    "1. It would be easy to see track the trends of test error decay rate in 2D like Fig2. from our paper. \n",
    "3. `Y` from the composition of multiple random kernels would end up as stable state (constant). That is, all the data will be almost same each other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# initialize variables and sample GP X\n",
    "full_N = 11500\n",
    "d_in = 15 #input dimension for X\n",
    "d_mid = 3 #dimension for the latent dataset Z\n",
    "d_out = 20 #output dimension for Y\n",
    "\n",
    "nu_g =  15 # differentiability of 1st kernel\n",
    "nu_h =  2 # differentiability of 2nd kernel\n",
    "############\n",
    "\n",
    "X = np.random.randn(full_N, d_in)  \n",
    "Z = matern_Kernel(X, nu = nu_g).sample(d_mid)\n",
    "Y = matern_Kernel(Z, nu = nu_h).sample(d_out, Tikhonov = True)\n",
    "\n",
    "pd.to_pickle(X, os.getcwd() + \"/sampled_kernels/\" + \"X_g_Matern7_{nu_g}_{nu_h}_{N}.pkl\".format(nu_g = nu_g, nu_h = nu_h, N=full_n))\n",
    "pd.to_pickle(Y, os.getcwd() + \"/sampled_kernels/\" + \"Y_g_Matern7_{nu_g}_{nu_h}_{N}.pkl\".format(nu_g = nu_g, nu_h = nu_h, N=full_n))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice $\\exists$ an argument `Tikhonov` in `Y`, but not in `Z`.\n",
    "\n",
    "This `Tikhonov` means Tikhonov regularization, which add a very small constant to the diag($K$). \n",
    "\n",
    "Why do we need this? As we know, our computed kernels should be postive definite. **However**, precision issues in modern computer simply ignore very small numbers after certain floating point. This makes very similar two data points exactly same, which also leads linearly dependecy -> 0 eigvalues -> not positive definite -> cannot compute Matern kernel =[\n",
    "\n",
    "Without this regularization, non positive definite kernels would like to occur more frequently. Notice `d_mid` is 3, which is more vulnerable to this issue when we try to compute `K_h`, the second Mat√©rn kernel computed from `Z`. (bigger dimension of data is likely avoid those coincidence of having very very similar values)\n",
    "\n",
    "This is the another reason that we only simulated the composition of two kernels. Compositioning more kernels would be more vulnerable to the non-positive definite issue due to the precision problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------\n",
    "Great! Now we need to sample all the combination of differentiability in the range [0.5,10]√ó[0.5,10]. The following code will sample 400 different combination of `X` and `Y`. Recall that due to time complexity and space complexity, running all the simulation would not be feasible for large $N$. This will take several hundreds of hours to finish, so we uploaded sampled kernels on Google Drive with more cases. Please download the kernels from our Drive instead of computing the almost exactly same task as we mentioned on our `Readme.md`for our üåé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differentiability [0.5,10]√ó[0.5,10]; 0.5 increment\n",
    "full_N = 52500\n",
    "\n",
    "#differentiability from 0.5 1.0 1.5 ... 19.5 20.0\n",
    "num_dif = np.arange(20)/2 + 0.5\n",
    "\n",
    "for n in list(itertools.product(num_dif, num_dif)):\n",
    "    nu_g = 0.5 + n[0]\n",
    "    nu_h = 0.5 + n[1]\n",
    "    \n",
    "    if not os.path.exists(os.getcwd() + \"/sampled_kernels/\" + \"X_g_Matern7_{nu_g}_{nu_h}_50000.pkl\".format(nu_g = n[0], nu_h = n[1])):\n",
    "        df = pd.DataFrame()\n",
    "        df.to_pickle(os.getcwd() + \"/sampled_kernels/\" + \"X_g_Matern7_{nu_g}_{nu_h}_50000.pkl\".format(nu_g = n[0], nu_h = n[1]))\n",
    "        \n",
    "        nu_g = 0.5 + n[0]\n",
    "        nu_h = 0.5 + n[1]\n",
    "        \n",
    "        X = np.random.randn(full_N, d_in)  \n",
    "        Z = matern_Kernel(X, nu = nu_g).sample(d_mid)\n",
    "        try :\n",
    "            Y = matern_Kernel(Z, nu = nu_h).sample(d_out)\n",
    "        \n",
    "        except:\n",
    "            Y = matern_Kernel(Z, nu = nu_h).sample(d_out, Tikhonov = True)\n",
    "        \n",
    "        pd.to_pickle(X, os.getcwd() + \"/sampled_kernels/\" + \"X_g_Matern7_{nu_g}_{nu_h}_50000.pkl\".format(nu_g = n[0], nu_h = n[1]))\n",
    "        pd.to_pickle(Y, os.getcwd() + \"/sampled_kernels/\" + \"Y_g_Matern7_{nu_g}_{nu_h}_50000.pkl\".format(nu_g = n[0], nu_h = n[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
